{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has notes and thoughts with most recent chapters in the beginning, and older thoughts, as you read on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C can now be seen live at work in [This Notebook](A2C_Curriculum.ipynb). It failed to produce a good policy. Indeed, it produces uniform distributions even from initially well-performing policies (from imitation learning) - which is the worst outcome.\n",
    "\n",
    "### Potential issues\n",
    "#### Frustration\n",
    "Incoming gradients from the losing side may increase the entropy such that the policy essentially creates a uniform distribution. \n",
    "\n",
    "Frustration may arise from the *critic* not being consistent yet. It's definitely worth looking at the quality of the *critic*'s advantages. On any single trajectory, the perfect critic would match the alernating bellmann scheme.\n",
    "\n",
    "Frustration will most likely arise from the alternating scheme in general, as negative and positive signals are evenly distributed. \n",
    "\n",
    "It would be interesting to report the entropy as a metric.\n",
    "\n",
    "One quick win could come from switching off the losing signal totally. But still, then the value signal will be noisy enough to provide almost arbitrary feedback also for the winner's moves. \n",
    "\n",
    "#### Off-by-one\n",
    "We still have that mysterious off-by-one problem in function ```analyse_and_recommend```.\n",
    "We need to understand what it is to rule out that despite the hack, it still provides a wrong recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary conclusion\n",
    "I now believe that A2C can't be successful with such a hard problem. Not even a little performance increase from imitation-learned trajectories is possible. This should definitely provide a warning in view of my CFDS project: Keep it simple!\n",
    "\n",
    "### Next steps \n",
    "It would really be interesting to see the setup success on a less demanding problem - just to see it working. I have QLearning tutorial in [this notebook](RL_QLearning.ipynb), that might provide a solvable toy problem. \n",
    "Then it is certainly advisable to implement the openai gym interfaces.\n",
    "\n",
    "The Gomoku problem can still be solved. With the policy network and the RL algorithm in place, an actor-learner can be implemented with manageable effort. An actor-learner scheme has a great chance to succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on policy-advised UCT\n",
    "The currently best policy and value network snapshots are \n",
    "```\n",
    "policy_model = PolicyModel(board_size=19, n_blocks=10, \n",
    "                    n_layers=3, n_filters=32, \n",
    "                    activation='relu')\n",
    "policy_model.load_weights(\"./models/PolicyNet_1.0/cp-0003.ckpt\")\n",
    "\n",
    "value_model = ValueModel(19, 10, 3, 32, 'relu')\n",
    "value_model.load_weights(\"./models/ValueNet_3.0/cp-0001.ckpt\")\n",
    "```\n",
    "These networks have been trained on the P5K dataset of 4600 heuristic gameplays until convergence. See [this notebook](A2C_PolicyNetwork.ipynb) and [this notebook](A2C_ValueNetwork.ipynb) for details.\n",
    "\n",
    "The policy may advise the tree search for the initial distribution (Thus, it's no longer MCTS). The value function will return feedback instead the rollout or allow the rollout to stop before a final state is reached. \n",
    "\n",
    "To boost performance, we should use the Ray framework (I started with [this notebook](RayTutorial.ipynb), but didn't really get anywhere yet). \n",
    "\n",
    "The most compelling idea at the moment:\n",
    "- have a master actor to start parallel tree searches\n",
    "- the search actors submit policy eval requests to a dispatcher\n",
    "- the dispatcher collects requests until an NN actor becomes available,\n",
    "- the NN actor processes the requests in batch and returns the result off-line\n",
    "- the dispatcher actor dispatches the results to its clients\n",
    "- maybe the NN actors could address different GPUs?\n",
    "\n",
    "Check Norway-notes, too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Thinking fast and slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to skip RL in favour of the Alpha-Zero approach with a policy-advised tree search, possibly considering RL for some side-line improvement later. However, I'm still going to initialize the network with Imitation learning from my heuristic policy.\n",
    "\n",
    "Interestingly, a pretty similar approach has been suggested by [Anthony 2017](https://arxiv.org/pdf/1705.08439.pdf), independent of the research done by Deepmind. Would be interesting to compare the approaches.\n",
    "\n",
    "[This essay](http://www.moderndescartes.com/essays/deep_dive_mcts/) is the most concise and comprehensible piece on UCT. It refers to a *NeuralNet* to provide value estimates for child nodes. I want to start from that, as it also advises an approach to vectorization to massively improve the performance of the search algorithm. The above algorithm takes a single policy evaluation at the child nodes to estimate a parent's value. It'd be interesting to consider some fast policy to chase down four or five more moves and average their results. Another thing is the formula used for evaluation of the UCB. That's adding 1 to the denominators for stability (precondition for vectorizing the UCB calculation. But it's also omitting the exploration parameter and doesn't take the logarithm on the parent's number of simulations. [This Medium blog](https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238) has the correct formula and some more helpful explanations.\n",
    "\n",
    "We'll have the architecture derived in [LinesOfFive.ipynb](LinesOfFive.ipynb) learn by imitating the [HeuristicGomokuPolicy](HeuristicPolicy.py). The latter needs to have some function that maps the logic of method ```suggest``` into a learnable distribution.\n",
    "\n",
    "That should already create a pretty strong player. Additional steps would possibly include ideas from [Anthony 2017](https://arxiv.org/pdf/1705.08439.pdf) to have system 1 (the policy network) and system 2 (the UCT algorithm) learn from each other.\n",
    "\n",
    "[HeuristicPolicy.ipynb](HeuristicPolicy.ipynb) is now the starting point for creating initial training data. I still need to find out how to effectively reflect the results of the threat sequence search in the resulting action (move) distribution.\n",
    "\n",
    "I could start with implementing UCT with the heuristic policy and see how it does.\n",
    "\n",
    "Another hard thing is then the full documentation and operationalization of the entire quest. Providing an interactive interface to play with the algo. A web version of GO-UI also being able to run tournaments. Also, benchmarking my algo against the available players at the official Gomoku tournament site is to be considered.\n",
    "\n",
    "Last, not least, the entire thing should be presentable on various occasions, meetups, conferences, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final AI Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final actor should have an openai environment interface. It features a \n",
    "variable-depth policy-advised threat-search. This policy should be trained on \n",
    "threat sequences. It could start as a regular tactical policy that has an extra sense for threats. The other policy should remain strong on tactics. Well, that may not be easy to achieve. Both policies need to be DRL-trained. So the feedback from a trajectory would need to be distributed according to the various roles. May not be easy...;-( \n",
    "The tactical tree search will have a tree different from the threat search - obviously,\n",
    "so RL would come in two phases. Up to the start of a threat search, the tactical tree and policy would be trained in some manner. Then the threat sequence itself will make training examples for the ts policy. Good thing here: Each sub-sequence is just another threat sequence, so learning from threat sequences essentially becomes supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Resources\n",
    "\n",
    "The best resource I have found about policy gradients:\n",
    "\n",
    "Berkeley's Joshua Achiam's Lecture Slides\n",
    "\n",
    "[Achiam2017](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf)\n",
    "\n",
    "---\n",
    "Somewhat useful: Jonathan Hui's Blog about Natural Policy Gradient and TRPO\n",
    "\n",
    "[Hui2018](https://medium.com/@jonathan_hui/rl-natural-policy-gradient-actor-critic-using-kronecker-factored-trust-region-acktr-58f3798a4a93)\n",
    "\n",
    "---\n",
    "\n",
    "Also pretty readable: Berkeley's Sergey Levine's Lecture notes on Actor-Critic Algorithms:\n",
    "\n",
    "[Levine](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "The TRPO Paper:\n",
    "\n",
    "[Schulmann2017] (https://arxiv.org/pdf/1502.05477.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "Excellent overview over the algos in \"Towards Data Science\"\n",
    "\n",
    "[Huang2018-1](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287)\n",
    "\n",
    "---\n",
    "\n",
    "The most approachable code I've seen by now - and even TF2.0\n",
    "[Ring2019](http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/)\n",
    "\n",
    "---\n",
    "\n",
    "Soft Actor-Critic\n",
    "[Aarnoya2018](https://arxiv.org/pdf/1801.01290.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "Tensorflow estimators:\n",
    "\n",
    "[Cheng2017](https://arxiv.org/pdf/1708.02637.pdf)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
